# Assignment Feedback: Week 04 Dimensionality Reduction

**Student:** jerryc233333
**Raw Score:** 45/50 (90.0%)
**Course Points Earned:** 96.0

---

## Problem Breakdown

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Great job. You correctly applied t-SNE to MNIST and visualized results with a clear scatter plot colored by labels. Sensible parameters (perplexity, n_iter), proper colorbar, and labeling. This meets the exercise goal.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good use of t-SNE embeddings with KNN and you report accuracy, addressing performance. Code is coherent and uses prior data appropriately. Note: fitting t-SNE on train+test leaks test info; for stricter eval, embed train only (or use a method with transform).

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job. You used UMAP embeddings to train KNN and computed test accuracy correctly. Using 30D UMAP is fine, and your fit/transform/split/predict flow is appropriate. Full credit.

---

### Exercise 4 (15/20 = 75.0%)

**Part ex2-part1** (ex2-part1.code): 5/7 points

_Feedback:_ Good attempt: you apply PCA (2D/3D), fit KNN, and visualize components. However, you train and evaluate on the same data (X,y) and don’t use a train/test split or transform X_test, so accuracy is overestimated. Please transform test data and report test accuracy. Using PCA(.9) is

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Good job applying UMAP with 2D and 3D, training KNN on the embeddings, reporting accuracy, and visualizing—consistent with your prior PCA workflow. For future improvement, consider evaluating on a held-out set to avoid overfitting, but not required here.

**Part ex2-part3** (ex2-part3.answer): 3/6 points

_Feedback:_ You gave a thoughtful comparison, but you didn’t explore parameters (e.g., UMAP n_neighbors) or note that UMAP often performs better in low dimensions with low neighbors. Full credit requires mentioning that or showing such exploration.

---

### Exercise 1 (20/20 = 100.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Good job: you reduced to 2D with PCA and produced a clear scatter colored by digit labels with legend and labels. This fully meets the task. Note: random_state in PCA is unused unless svd_solver='randomized', but it doesn’t affect correctness here.

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Excellent work. You fit PCA with 40 components and plotted the percent variance explained for each, plus a useful cumulative curve. This meets the scree plot requirements and correctly uses the training data. No issues detected.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correct approach: fit PCA on full data, compute cumulative explained variance, and find minimal components to reach 95%. Works with prior data and meets the task. Nice job.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Good job. You used the Step 4 dimensionality (148 comps) to reduce and reconstruct digits, and visualized original vs reconstruction correctly. Approach is consistent with prior work and meets the objective. Using a custom plotting function is fine.

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Excellent work. You trained KNN with and without PCA, used PCA with n_components=0.80 to preserve ~80% variance, transformed train/test correctly, and compared accuracies. Reporting the number of components kept is a nice touch. Full credit.

---

## Additional Information

This feedback was automatically generated by the autograder.

**Generated:** 2025-10-28 19:51:43 UTC

If you have questions about your grade, please reach out to the instructor.